{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 일반적인 DataSet 준비\n",
    "trainframe = pd.read_csv(\"train_data.csv\")\n",
    "testframe = pd.read_csv(\"test_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    " Meta Learner를 위한\n",
    " Meta-train / meta-test용 \n",
    " Dataset을 준비하는 Class\n",
    "\"\"\"\n",
    "class Task(object):\n",
    "    \n",
    "    def __init__(self\n",
    "                     , all_classes\n",
    "                     , num_classes\n",
    "                     , num_instances):\n",
    "        \n",
    "    # 예시) task = Task(meta_train_classes, num_classes(5), num_instances(3))    \n",
    "        \n",
    "    self.all_classes = all_classes       # 준비된 전체 class [list]\n",
    "    self.num_classes = num_classes       # N-way\n",
    "    self.num_instances = num_instances   # K-shot\n",
    "    self.train_roots = []                # meta_train_img_src_path\n",
    "    self.meta_roots = []                 # meta_test_img_src_path\n",
    "    self.train_labels = []               # meta_test_img_index(y-val)\n",
    "    self.meta_labels = []                # meta_test_img_index(y-val)\n",
    "    samples_per_class = 20               # 각 class별로 준비된 image 개수, 준비 된 image이니, 고정 값\n",
    "    sampled_classes = random.sample(all_classes, num_classes)  # 해당 Task에서 선정된 label들 예를 들어 전체 class(label)의 subset\n",
    "    # ex) all_classes가 list(:20)일때, sample은 [14,17,5] 이런식, 해당 sample class가 해당 task의 target class (train, test로 분기)\n",
    "    label = 0\n",
    "    for c in sampled_classes:\n",
    "        cframe = trainframe.iloc[(c*samples_per_class):((c+1)*samples_per_class)] \n",
    "        # 이미지가 class 별로 순서 대로 들어있어서 위와같이 추출.\n",
    "        cframe.reset_index(inplace=True, drop=True)\n",
    "        paths = cframe[\"Path\"] # img path\n",
    "        # 특정 calss에 대해, meta-train, meta-test로 사용될 image index들 (여긴 20장으로 넉넉 하니깐..)\n",
    "        sample_idxs = np.random.choice(samples_per_class, samples_per_class, replace=False)\n",
    "        train_idxs = sample_idxs[:num_instances] # meta_train_img_index\n",
    "        meta_idxs = sample_idxs[num_instances:(num_instances*2)] \n",
    "        # meta_test_img_index ~ 여기선 support 개수와 query개수를 같게 했네.\n",
    "        # class는 고정되게, meta-train에 있던 class는 meta-test에도 있게 Restrict\n",
    "        for idx in train_idxs:\n",
    "            self.train_roots.append(paths[idx])\n",
    "            self.train_labels.append(label)\n",
    "        for idx in meta_idxs:\n",
    "            self.meta_roots.append(paths[idx])\n",
    "            self.meta_labels.append(label)\n",
    "        label+=1\n",
    "\n",
    "\"\"\"\n",
    " 실제 Train을 위한 \n",
    " Train / Test용 \n",
    " Dataset을 준비하는 Class\n",
    "\"\"\"\n",
    "class TestTask(object):\n",
    "    def __init__(self\n",
    "                   , all_classes\n",
    "                   , num_classes\n",
    "                   , num_instances\n",
    "                   , num_test_instances):\n",
    "    self.all_classes = all_classes\n",
    "    self.num_classes = num_classes\n",
    "    self.num_instances = num_instances\n",
    "    self.num_test_instances = num_test_instances # meta_test Data Point 개수\n",
    "    self.test_roots = []\n",
    "    self.train_roots = []\n",
    "    self.test_labels = []\n",
    "    self.train_labels = []\n",
    "    samples_per_class = 20\n",
    "    sampled_classes = random.sample(all_classes,num_classes)\n",
    "    label = 0\n",
    "\n",
    "    for c in sampled_classes:\n",
    "        cframe = testframe.iloc[((c-964)*samples_per_class):(((c+1)-964)*samples_per_class)]\n",
    "        cframe.reset_index(inplace=True, drop=True)\n",
    "        paths = cframe[\"Path\"]\n",
    "        sample_idxs = np.random.choice(samples_per_class, samples_per_class, replace=False)\n",
    "        train_idxs = sample_idxs[:num_instances]\n",
    "        test_idxs = sample_idxs[num_instances:(num_instances + num_test_instances)]\n",
    "        for idx in test_idxs:\n",
    "            self.test_roots.append(paths[idx])\n",
    "            self.test_labels.append(label)\n",
    "        for idx in train_idxs:\n",
    "            self.train_roots.append(paths[idx])\n",
    "            self.train_labels.append(label)\n",
    "        label+=1\n",
    "        \n",
    "\"\"\"\n",
    " Image DataLoading을 위한\n",
    " Util성 Data format용 Class\n",
    "\"\"\"        \n",
    "class MiniSet(Dataset):\n",
    "    def __init__(self\n",
    "                    , fileroots\n",
    "                    , labels\n",
    "                    , transform):\n",
    "        self.fileroots = fileroots\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.fileroots)\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        img = Image.open(self.fileroots[idx])\n",
    "        img = self.transform(img)\n",
    "        return img,self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "transform = transforms.Compose([  transforms.Resize((28,28))\n",
    "                                , transforms.ToTensor()      ])\n",
    "\n",
    "\"\"\"\n",
    " 앞선, meta learner용 Task Class와\n",
    " Image load용 Class를 토대로 \n",
    " Meta Learner용 (inner loop)\n",
    " Dataset loader\n",
    "\"\"\"\n",
    "def get_loaders(task):\n",
    "    loaders = {}\n",
    "    train_fileroots = task.train_roots     # meta_train\n",
    "    train_labels = task.train_labels       # meta_train\n",
    "    meta_fileroots = task.meta_roots       # meta_test\n",
    "    meta_labels = task.meta_labels         # meta_test\n",
    "    \n",
    "    trainloader = DataLoader(  MiniSet(train_fileroots,train_labels,transform)\n",
    "                             , batch_size=len(train_fileroots)\n",
    "                             , shuffle=True)\n",
    "    metaloader = DataLoader(   MiniSet(meta_fileroots,meta_labels,transform)\n",
    "                             , batch_size=len(meta_fileroots)\n",
    "                             , shuffle=True)\n",
    "    loaders[\"train\"] = trainloader    # meta-train (support)\n",
    "    loaders[\"meta\"] = metaloader      # meta-test  (query)\n",
    "    return loaders\n",
    "\n",
    "def get_test_loaders(task):\n",
    "    loaders = {}\n",
    "    test_fileroots = task.test_roots\n",
    "    test_labels = task.test_labels\n",
    "    train_fileroots = task.train_roots\n",
    "    train_labels = task.train_labels\n",
    "    testloader = DataLoader(  MiniSet(test_fileroots,test_labels,transform)\n",
    "                            , batch_size=len(test_fileroots)\n",
    "                            , shuffle=True)\n",
    "    trainloader = DataLoader(  MiniSet(train_fileroots,train_labels,transform)\n",
    "                             , batch_size=len(train_fileroots)\n",
    "                             , shuffle=True)\n",
    "    loaders[\"train\"] = trainloader   # origin-train\n",
    "    loaders[\"test\"] = testloader     # origin-test\n",
    "    return loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseNet(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(BaseNet,self).__init__()\n",
    "        self.features = nn.Sequential(OrderedDict([\n",
    "                ('conv1', nn.Conv2d(1, 64, 3)),\n",
    "                ('bn1', nn.BatchNorm2d(64, momentum=1, affine=True)),\n",
    "                ('relu1', nn.ReLU(inplace=True)),\n",
    "                ('pool1', nn.MaxPool2d(2,2)),\n",
    "                ('conv2', nn.Conv2d(64,64,3)),\n",
    "                ('bn2', nn.BatchNorm2d(64, momentum=1, affine=True)),\n",
    "                ('relu2', nn.ReLU(inplace=True)),\n",
    "                ('pool2', nn.MaxPool2d(2,2)),\n",
    "                ('conv3', nn.Conv2d(64,64,3)),\n",
    "                ('bn3', nn.BatchNorm2d(64, momentum=1, affine=True)),\n",
    "                ('relu3', nn.ReLU(inplace=True)),\n",
    "                ('pool3', nn.MaxPool2d(2,2))]))    \n",
    "        self.add_module('fc', nn.Linear(64,num_classes))\n",
    "\n",
    "    def forward(self,x,weights=None):\n",
    "        if weights == None:  \n",
    "            output = self.features(x)\n",
    "            output = output.view(-1, 64)\n",
    "            output = self.fc(output)\n",
    "        else:\n",
    "            x = F.conv2d(x, weights['meta_learner.features.conv1.weight'], weights['meta_learner.features.conv1.bias'])\n",
    "            x = F.batch_norm(x, weights['meta_learner.features.bn1.running_mean'], \n",
    "                                weights['meta_learner.features.bn1.running_var'],\n",
    "                                weights['meta_learner.features.bn1.weight'],\n",
    "                                weights['meta_learner.features.bn1.bias'],momentum=1,training=True)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, kernel_size=2, stride=2) \n",
    "        x = F.conv2d(x, weights['meta_learner.features.conv2.weight'], weights['meta_learner.features.conv2.bias'])\n",
    "        x = F.batch_norm(x, weights['meta_learner.features.bn2.running_mean'], \n",
    "                            weights['meta_learner.features.bn2.running_var'],\n",
    "                            weights['meta_learner.features.bn2.weight'],\n",
    "                            weights['meta_learner.features.bn2.bias'],momentum=1,training=True)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, kernel_size=2, stride=2) \n",
    "        x = F.conv2d(x, weights['meta_learner.features.conv3.weight'], weights['meta_learner.features.conv3.bias'])\n",
    "        x = F.batch_norm(x, weights['meta_learner.features.bn3.running_mean'], \n",
    "                            weights['meta_learner.features.bn3.running_var'],\n",
    "                            weights['meta_learner.features.bn3.weight'],\n",
    "                            weights['meta_learner.features.bn3.bias'],momentum=1,training=True)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, kernel_size=2, stride=2) \n",
    "        x = x.view(x.size(0), 64)\n",
    "        output = F.linear(x, weights['meta_learner.fc.weight'], weights['meta_learner.fc.bias'])\n",
    "        out = F.log_softmax(output, dim=1)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MetaLearner(nn.Module):\n",
    "    def __init__(self,num_classes):\n",
    "        super(MetaLearner, self).__init__()\n",
    "        self.meta_learner = BaseNet(num_classes)\n",
    "\n",
    "    def forward(self, x, mod_weights=None):\n",
    "        if mod_weights==None:\n",
    "            out = self.meta_learner(x)\n",
    "        else:\n",
    "            out = self.meta_learner(x, mod_weights)\n",
    "        return out\n",
    "  \n",
    "    def clone_state_dict(self):\n",
    "        cloned_state_dict = {key: val.clone()for key, val in self.state_dict().items()}\n",
    "        return cloned_state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "    Meta Learner 학습\n",
    "\"\"\"\n",
    "def train_single_task(net, lr, loaders, num_updates, loss_metric):\n",
    "    net.train()                             # dropout, batch norm등.\n",
    "    trainloader = loaders[\"train\"]          # set meta-train loader (support)\n",
    "    x,y = trainloader.__iter__().next()     # image, label\n",
    "    x.to(device)\n",
    "    y.to(device)\n",
    "    \n",
    "    output = net(x)                          # y^ : predict\n",
    "    loss = loss_metric(output, y)            # Negative log likelihood loss  \n",
    "     \n",
    "    # inner function. 우리는 net을 재사용 하니깐 이렇게 manually 초기화 해줘야 됨.\n",
    "    def zero_grad(params):\n",
    "        for p in params:\n",
    "            if p.grad is not None:\n",
    "                p.grad.zero_()\n",
    "    \n",
    "    # weight 초기화\n",
    "    zero_grad(net.parameters())\n",
    "    # 해당 step의 gradient 추출\n",
    "    grads = torch.autograd.grad(loss, net.parameters(), create_graph=True)\n",
    "    # 해당 step의 network 복사\n",
    "    mod_state_dict = net.clone_state_dict()\n",
    "    mod_weights = OrderedDict()\n",
    "    \n",
    "    for (k,v), g in zip(net.named_parameters(), grads):\n",
    "        mod_weights[k] = v - lr*g # param update를 이렇게 manually .. --> 얘가 phi_i가 됨.\n",
    "        mod_state_dict[k] = mod_weights[k]\n",
    "    \n",
    "    # 해당 meta learner 학습 loop를 반복 ~ 그러나 논문에선 num_updates가 1 이므로 해당 내용은 동작 x\n",
    "    for i in range(1, num_updates):\n",
    "        output = net(x, mod_state_dict)\n",
    "        loss = loss_metric(output, y)\n",
    "        zero_grad(mod_weights.values())\n",
    "        grads = torch.autograd.grad(loss,mod_weights.values(),create_graph=True)\n",
    "        for (k,v), g in zip(mod_weights.items(),grads):\n",
    "            mod_weights[k] = v - lr*g \n",
    "            mod_state_dict[k] = mod_weights[k]\n",
    "            \n",
    "    # 해당 task로 조정된 network return\n",
    "    return mod_state_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>[Train handling]</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train( net                   # base net ~ torch.nn \n",
    "         , meta_train_classes    # [0,1,2,3,4...N] Encoding된 class(label) \n",
    "         , meta_optimiser        # Adam\n",
    "         , loss_metric           # nn.NLLLoss()         \n",
    "                                 #   : Negative log likelihood loss \n",
    "                                 #     -P(x)LogQ(x), p_dist : ground truth, Q_dist : predict\n",
    "                                 #     CE에서 softmax가 따로 필요 없으니..\n",
    "         , num_classes           # 5 way\n",
    "         , num_instances         # 3 shot\n",
    "         , num_tasks             # outer loop count ~ meta leaner의 looping 횟수 (10회)\n",
    "         , lr                    # hyper param\n",
    "         , meta_lr               # hyper param\n",
    "         , num_inner_updates     # hyper param\n",
    "                                 # : outer loop 반복 횟수, 논문에서 1번 ~ 적은게 더 gaussian에 근접하다.\n",
    "         , num_epochs ):         # hyper param\n",
    "\n",
    "    total_loss = 0  \n",
    "    print_every = 100\n",
    "    plot_every = 2\n",
    "    meta_losses = []\n",
    "    \n",
    "    # meta train loop\n",
    "    for epoch in range(1, num_epochs+1):\n",
    "        \n",
    "        # Meta learner용 dict들 초기화\n",
    "        state_dicts = []\n",
    "        loaders_list = []\n",
    "        \n",
    "        # 몇개의 task를 돌릴것이냐, outer loop ~ meta leaner의 looping 횟수\n",
    "        for n in range(num_tasks):\n",
    "            # 앞서 만든 Task class를 통해 task 도출 \n",
    "            # Train class candidate에서 5 Way 3 Shot image path 추출\n",
    "            task = Task(meta_train_classes, num_classes, num_instances)\n",
    "            # 위의 task instance를 base로 pytorch loader 생성\n",
    "            loaders = get_loaders(task)\n",
    "            \"\"\"\n",
    "                Meta learning 부분,\n",
    "                Task들을 순회하며, weight 도출\n",
    "            \"\"\"\n",
    "            # meta learner 1 task 돌린 결과를 담아 놓음.\n",
    "            d = train_single_task(net, lr, loaders, num_inner_updates, loss_metric) # --> phi_i\n",
    "            # task별로 weight와 loader를 저장\n",
    "            state_dicts.append(d)\n",
    "            loaders_list.append(loaders)\n",
    "\n",
    "        metaloss=0\n",
    "        for n in range(num_tasks):\n",
    "            loaders = loaders_list[n] # 앞서 task별로 수행했던 loader를 다시 추출 meta-test를 위해\n",
    "            metaloader = loaders[\"meta\"] # meta-test data\n",
    "            x, y = metaloader.__iter__().next()\n",
    "            x.to(device)\n",
    "            y.to(device)\n",
    "            \"\"\"\n",
    "                순회한 task별 loss 산출\n",
    "            \"\"\"\n",
    "            d = state_dicts[n] # 해당 task index에서 수행된 결과 network\n",
    "            output = net(x, d) # 여기서 net은 신규 network이고 각 task별 meta-learner의 결과 network이 initial weight로 들어감.\n",
    "            loss = loss_metric(output,y) # 위의 initial weight base의 test loss 산출\n",
    "            metaloss += loss # 각 task들의 loss를 sum\n",
    "    \n",
    "            \n",
    "        # 전체 task의 평균 loss로 \n",
    "        # 즉, \n",
    "        #    여러개의 task들이 (class combination) 고르게 반영된 방향으로\n",
    "        #    weight가 없데이트 되게한다.\n",
    "        #    여러 task들의 평균 loss로 (metaloss var에 담긴) weight update가 되기에 \n",
    "        metaloss /= float(num_tasks) \n",
    "        meta_optimiser.zero_grad()\n",
    "        total_loss += metaloss.item()\n",
    "        metaloss.backward()\n",
    "        meta_optimiser.step() # 실제 weight 갱신 !!\n",
    "\n",
    "        if epoch % print_every == 0:\n",
    "            print(\"{}/{}. loss: {}\".format(epoch, num_epochs, total_loss / plot_every))\n",
    "        if epoch%plot_every==0:\n",
    "            meta_losses.append(total_loss/plot_every)\n",
    "            total_loss = 0\n",
    "        if (epoch%20)==0:\n",
    "            print(\"Epoch \"+str(epoch)+\" completed.\")\n",
    "    return meta_losses, net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> [Learning - main] </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    " 실제 train을 실행\n",
    "\"\"\"\n",
    "loss_metric = nn.NLLLoss()         # Negative log likelihood loss \n",
    "                                   # -P(x)LogQ(x), p_dist : ground truth, Q_dist : predict\n",
    "                                   # CE에서 softmax가 따로 필요 없으니..\n",
    "num_classes = 5                    # 5 way\n",
    "net = MetaLearner(num_classes)\n",
    "lr = 1e-1                          # hyper param\n",
    "meta_lr = 1e-3                     # hyper param\n",
    "meta_optimizer = torch.optim.Adam(net.parameters(), lr=meta_lr) # meta learning\n",
    "num_instances = 3                  # 3 shot\n",
    "num_tasks = 10                     # outer loop count ~ meta leaner의 looping 횟수\n",
    "num_inner_updates = 1              # outer loop 반복 횟수, 논문에서 1번 ~ 적은게 더 gaussian에 근접하다.\n",
    "num_epochs = 1000                  # hyper param\n",
    "train_classes = np.max(trainframe['Label']) # 전체 class, Task별이 아닌 준비된 전체\n",
    "train_classes = list(np.arange(train_classes))\n",
    "\n",
    "metalosses, net = train( net\n",
    "                       , train_classes\n",
    "                       , meta_optimizer\n",
    "                       , loss_metric\n",
    "                       , num_classes\n",
    "                       , num_instances\n",
    "                       , num_tasks\n",
    "                       , lr\n",
    "                       , meta_lr\n",
    "                       , num_inner_updates\n",
    "                       , num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(outputs, labels):\n",
    "    outputs = np.argmax(outputs, axis=1)\n",
    "    return np.sum(outputs == labels) / float(labels.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate( net\n",
    "            , test_classes\n",
    "            , task_lr\n",
    "            , num_classes = 5\n",
    "            , num_steps = 100\n",
    "            , num_eval_updates = 3):\n",
    "    losses = []\n",
    "    acc_list = []\n",
    "    \n",
    "    for step in np.arange(num_steps):\n",
    "        task = TestTask(test_classes,num_classes=5,num_instances=3,num_test_instances=10)\n",
    "        loaders = get_test_loaders(task)\n",
    "        trainloader, testloader = loaders[\"train\"], loaders[\"test\"]\n",
    "        x_train,y _train = trainloader.__iter__().next()\n",
    "        x_test,y_test = testloader.__iter__().next()\n",
    "        x_train.to(device)\n",
    "        y_train.to(device)\n",
    "        x_test.to(device)\n",
    "        y_test.to(device)\n",
    "\n",
    "    cloned_net = copy.deepcopy(net)\n",
    "    optim = torch.optim.SGD(cloned_net.parameters(), lr=task_lr)\n",
    "    \n",
    "    for _ in range(num_eval_updates):\n",
    "        y_train_pred = cloned_net(x_train)\n",
    "        loss = loss_metric(y_train_pred,y_train)\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "    \n",
    "    y_test_pred = cloned_net(x_test)\n",
    "    loss = loss_metric(y_test_pred,y_test)\n",
    "    losses.append(loss)\n",
    "    y_test_pred = y_test_pred.data.cpu().numpy()\n",
    "    y_test = y_test.data.cpu().numpy()\n",
    "    acc = accuracy(y_test_pred,y_test)\n",
    "    acc_list.append(acc)\n",
    "    return acc_list,losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "#test_classes=np.max(testframe['Label'])-np.min(testframe['Label'])\n",
    "test_classes = list(np.arange(np.min(testframe['Label']),np.max(testframe['Label']+1)))\n",
    "acc_list,losses = evaluate(net, test_classes, task_lr=1e-1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
